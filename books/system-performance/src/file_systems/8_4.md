# Architecture

1. This section introduces generic and specific file system architecture, beginning with the I/O stack, VFS, file system caches and features, common file system types, volumes, and pools. Such background is useful when determining which file system components to analyze and tune. For deeper internals and other file system topics, refer to source code, if available, and external documentation. Some of these are listed at the end of this chapter.

## File System I/O Stack

1. The path of I/O from applications and system libraries moves through syscalls and the kernel.

2. Raw I/O refers to system calls that directly communicate with the disk device subsystem.
3. File system I/O travels via the Virtual File System (VFS) and the file system, including direct I/O that bypasses the file system cache.
4. You are correct in highlighting the distinction between the standard file system interface and raw I/O at the disk level. In the example I provided earlier, the system calls (`open`, `read`, `write`, and `close`) indeed operate through the file system layer (e.g., ext4 on Linux), which is not the same as performing raw disk I/O.

   Raw I/O bypasses the file system cache and directly interacts with the disk, which can be necessary for applications that require precise control over data placement on the storage medium or need to ensure that data is immediately written to disk without any caching interference.

   ### Performing Raw I/O on Linux

   On Linux, raw I/O can be achieved by accessing devices directly through their device files (e.g., `/dev/sda`) or by using specific system calls that allow bypassing the buffer cache, such as using the `O_DIRECT` flag with the `open` system call.

   Here's how you might modify the previous example to perform raw I/O on Linux:

   #### Using O_DIRECT

   ```c
   #include <fcntl.h>
   #include <unistd.h>
   #include <stdio.h>
   #include <stdlib.h>
   #include <string.h>
   #include <errno.h>
   
   int main() {
       int fd;
       ssize_t bytes_read, bytes_written;
       const char* path = "/dev/sda";  // Be extremely careful with this!
       char* buffer;
       size_t buf_size = 4096;  // Use a multiple of the disk block size
   
       // Allocate aligned memory to use with O_DIRECT
       posix_memalign((void **)&buffer, 4096, buf_size);
   
       // Open the device file directly with O_DIRECT to bypass the buffer cache
       fd = open(path, O_DIRECT | O_RDONLY);
       if (fd < 0) {
           perror("Failed to open device file");
           free(buffer);
           return 1;
       }
   
       // Read directly from the disk device
       bytes_read = read(fd, buffer, buf_size);
       if (bytes_read < 0) {
           perror("Failed to read from device");
           close(fd);
           free(buffer);
           return 1;
       }
   
       printf("Read %zd bytes directly from %s.\n", bytes_read, path);
   
       // Cleanup
       close(fd);
       free(buffer);
   
       return 0;
   }
   ```

   ### Key Points and Cautions:
   - **O_DIRECT**: This flag can be used to minimize or eliminate cache effects of the I/O to and from the device. It provides more control over I/O, but it requires that all I/O operations, buffers, and file offsets must be aligned to the filesystem's block size.
   - **Device Files**: Accessing `/dev/sda` or similar device files requires root privileges and extreme caution, as incorrect operations can corrupt your system or lead to data loss.
   - **Memory Alignment**: When using `O_DIRECT`, buffers must be memory-aligned to the disk's block size (commonly 512 bytes or 4KB). This example uses `posix_memalign()` to ensure the buffer is aligned.

   This kind of operation is typically used in very specific scenarios like specialized database management systems or certain types of data recovery tools, where control over physical disk access is crucial. Always test such code in a controlled environment (e.g., using a virtual machine or a designated test device) to avoid accidental data loss.

## VFS

1. The Virtual File System (VFS) interface offers a unified interface for different file system types.
2. VFS was initially developed in SunOS and has become the universal standard for abstracting file systems.
3. The Linux VFS interface terminology might be confusing as it uses traditional Unix terms like inodes and superblocks for VFS objects, which are memory-only structures.
4. On-disk data structures in Linux are often named with a prefix indicating their file system type, such as `ext4_inode` and `ext4_super_block`.
5. The VFS interface can also be used to measure the performance of any file system through OS-supplied statistics or instrumentation techniques.

## File System Caches

1. Unix originally had only the buffer cache to improve the performance of block device access. Nowadays, Linux has multiple different cache types.

### Buffer Cache

1. Unix originally implemented a buffer cache at the block device interface to cache disk blocks, which was a separate, fixed-size cache.
2. The introduction of the page cache later created challenges in balancing workloads and managing the overheads from double caching and synchronization between the buffer and page caches.
3. These issues were largely resolved by SunOS with the introduction of the unified buffer cache, which incorporates the buffer cache within the page cache.
4. Since the Linux 2.4 kernel, Linux has also adopted this approach, storing the buffer cache in the page cache to avoid the problems of double caching and synchronization overhead.
5. The term "buffer cache" still exists and is used in Linux, especially in observability tools like the `free` command, although its functionality has evolved to improve block device I/O performance.
6. Unlike its initial implementation, the size of the buffer cache in Linux is now dynamic and can be observed via the `/proc` filesystem.

### Dentry Cache

1. The Dentry Cache (Dcache) in Linux enhances path name lookup performance by storing mappings from directory entries (struct dentry) to VFS inodes, which is similar to the directory name lookup cache (DNLC) used in earlier Unix systems.

2. When traversing a pathname, the Dcache allows each lookup to directly access inode mappings through a hash table, avoiding the need to search through directory contents. This hash table is optimized for quick and scalable searches, keyed by the parent dentry and directory entry name.
3. Performance improvements include the implementation of the read-copy-update-walk (RCU-walk) algorithm, which reduces the overhead of updating dentry reference counts during high-frequency pathname lookups on multi-CPU systems. If a required dentry is not found in the cache, the system reverts to the slower reference-count based walk (ref-walk).
4. The Dcache also supports negative caching, which caches failed lookups for non-existent entries, improving the performance when searches (such as those for shared libraries) do not find the intended files.
5. The size of the Dcache is dynamic, adjusting with the system's memory needs by shrinking entries based on their last used status (Least Recently Used, or LRU algorithm). The current size of the Dcache can be observed through the `/proc` filesystem.
6. The `struct dentry` is a structure used in the Linux kernel to represent directory entries in the filesystem. It is a core component in the management of the filesystem's directory cache, helping to efficiently map between directory entries and their corresponding inodes.

   Here is a simplified outline of the key members typically found in the `struct dentry` structure:

   1. **d_inode**: A pointer to the inode this entry refers to. This provides the actual content and metadata of the file.

   2. **d_parent**: A pointer to the parent `dentry`. This helps in navigating the filesystem hierarchy.

   3. **d_name**: The name of the directory entry. This is usually represented as a `struct qstr` (qualified string), which includes the name length and the hash value to speed up comparisons and lookups.

   4. **d_list**: List links for other `dentry` structures. These are used to chain them into various lists, such as free lists or LRU lists, facilitating quick lookups and management.

   5. **d_lock**: A spinlock to protect the dentry's fields from concurrent access, ensuring thread safety.

   6. **d_count**: A usage count (reference count) that indicates how many processes are using this dentry.

   7. **d_flags**: Various flags that provide information on the state and behavior of the dentry, such as whether it has been mounted on, is unhashed, or is in use.

   8. **d_sb**: A pointer to the superblock of the filesystem to which this entry belongs. This ties the entry to a specific filesystem.

   9. **d_op**: Pointer to the dentry operations table, which includes methods for managing the dentry-specific operations.

   10. **d_lru**: List head for including the dentry in the least recently used list, which is used for dentry eviction from the cache.

   11. **d_subdirs** and **d_child**: These members are used to manage a list of child dentries, making the traversal of directory hierarchies possible.

   12. **d_mounted**: An integer used to check if the dentry is a mount point.

   These members together provide the necessary functionalities for filesystem navigation, manipulation, and management in the Linux kernel. The actual implementation and specific members can vary depending on the Linux kernel version and configuration.
7. Sure, I can describe a hypothetical scenario of a directory in a Linux filesystem and detail the `struct dentry` members for that directory.

   Let's consider a directory named `example` located at `/home/user/example`. We'll explore the relevant `struct dentry` members for this directory:

   1. **d_inode**: Points to the inode associated with `/home/user/example`. This inode contains metadata such as permissions (e.g., read, write, execute), ownership, timestamps (creation, modification), and pointers to the data blocks where the actual files and subdirectories are stored.

   2. **d_parent**: Points to the dentry of `/home/user`, the parent directory. This linkage helps in navigating upwards in the filesystem hierarchy.

   3. **d_name**: Contains the name of the directory, `example`. The `struct qstr` (qualified string) that represents this name includes the string "example", its length, and a hash value for quick comparisons.

   4. **d_list**: This member is used to link `/home/user/example` in various lists within the filesystem, such as hash chains or free lists.

   5. **d_lock**: A spinlock that protects this dentry from concurrent modifications, crucial in a multitasking environment where multiple processes might attempt to modify filesystem structures concurrently.

   6. **d_count**: Shows how many processes are currently using this dentry. If `d_count` is greater than zero, the dentry is in use.

   7. **d_flags**: Holds flags indicating the state of the dentry, such as `DCACHE_CONNECTED` which indicates that the dentry is connected to the dentry cache tree.

   8. **d_sb**: Points to the superblock of the filesystem containing `/home/user/example`. This superblock structure contains information about the entire filesystem, like its type, size, status, and operations.

   9. **d_op**: Pointer to the dentry operations for this particular filesystem. These operations might include methods to delete, rename, or compare dentries.

   10. **d_lru**: Links this dentry into the least recently used (LRU) list, which is used for managing cache eviction when memory is low.

   11. **d_subdirs** and **d_child**: Manage lists of child dentries. For example, if `/home/user/example` contains subdirectories or files, they would be linked via these lists.

   12. **d_mounted**: If `example` were a mount point for another filesystem, this would be indicated by `d_mounted`.

8. The dentry cache, also known as the "directory entry cache," is a crucial part of the Linux kernel's filesystem architecture. Its primary role is to cache directory entries (dentries) to speed up filesystem navigation by reducing the number of disk accesses required when files are accessed repeatedly.

   ### What Does the Dentry Cache Cache?

   The dentry cache caches entire `struct dentry` structures. This includes all its members such as `d_inode`, `d_parent`, `d_name`, and others mentioned previously. By caching the entire structure, the kernel can quickly resolve paths and check permissions without needing to repeatedly read this information from the disk.

   ### Structure of the Dentry Cache

   The dentry cache is not a simple linear structure but rather a complex system involving several components that work together to manage cached entries efficiently. Here’s a general idea of the structure and operation:

   1. **Hash Table**: At the core of the dentry cache is a hash table. This table hashes dentries based on their names and parent directory pointers, allowing for fast lookup, insertion, and removal operations.

   2. **Lists**:
      - **LRU (Least Recently Used) List**: This is used to track and manage the age of dentries in the cache. Older dentries can be reclaimed if memory is needed elsewhere.
      - **Free List**: Contains dentries that are not currently used but are kept around for potential reuse to avoid the overhead of creating new dentries.
      - **Used List**: Dentries that are currently in use are kept in this list.

   3. **Reference Counting**: Each dentry has a reference count (`d_count`), which tracks how many kernel components are using it. A dentry can't be removed from the cache as long as its reference count is greater than zero.

   4. **Locking**: To manage concurrent access to dentries, the cache uses various locks. For instance, the `d_lock` in each dentry helps synchronize access to individual entries, while global locks or more complex mechanisms might be used to manage access to the cache structure itself.

   ### Example of Cache Operation

   When a file is accessed:
   1. The kernel constructs the path from the root, checking each component against the cache.
   2. If a dentry is found in the cache, it is quickly retrieved and used.
   3. If not, a new dentry is created, added to the hash table, and linked into the appropriate lists.

   The efficiency of the dentry cache dramatically reduces the overhead associated with filesystem operations, particularly in environments where directories and files are accessed frequently. The structure and mechanisms of the dentry cache are optimized for both speed and minimal memory usage, balancing between quick access and economical memory use.
9. The idea that the kernel reads all the inode information from disk into memory at system boot is not exactly how it operates, mainly because such an approach would be highly inefficient for several reasons:

   1. **Memory Usage**: Modern filesystems can contain millions of files and directories. Loading all associated inodes and dentries into memory at boot would consume a significant amount of RAM, which is impractical, especially on systems with limited memory resources.

   2. **Performance**: Reading all inode data from the disk at boot time would significantly slow down the boot process and is unnecessary since many files might not be needed during a typical session.

   3. **Dynamic Nature**: Filesystems are dynamic, with files and directories constantly being created, modified, and deleted. Static loading at boot wouldn't account for these changes.

   ### How Dentries are Actually Created

   Instead, the kernel creates dentries dynamically as they are needed during the operation of the system. Here’s a simplified explanation of the process:

   1. **Path Lookup**: When a process accesses a file or directory, the kernel performs a path lookup. It starts from the root (or another starting point, such as the current directory) and traverses the path component by component.

   2. **Dentry Cache Check**: For each component of the path, the kernel first checks the dentry cache to see if a dentry for this component (directory or file) already exists.
      - If it exists in the cache and is valid, the kernel uses this dentry.
      - If it does not exist or has been evicted from the cache, the kernel needs to read the corresponding inode from the disk.

   3. **Reading from Disk**: If a dentry is not found in the cache:
      - The kernel reads the inode information from the disk for the required directory or file.
      - It then creates a new dentry, populates it with the inode data, and inserts it into the dentry cache. This new dentry now contains pointers to its parent, its name, and other necessary metadata.

   4. **Caching**: Once a dentry is created, it is cached so that subsequent accesses to the same file or directory can bypass the slow disk read operation. The cached dentries are managed based on their usage, with least recently used items being candidates for eviction when memory is needed elsewhere.

   This approach allows the kernel to manage filesystem data efficiently, only loading what is necessary when it is necessary, and keeping frequently accessed items quickly accessible in the dentry cache. This dynamic handling is crucial for maintaining performance and scalability in a multitasking operating system like Linux.
10. Yes, that's correct! When you access a file like `/home/mralians/dbus.sh`, the Linux kernel performs a step-by-step traversal of the directory path, starting from the root (`/`). At each step, the kernel performs a series of operations to resolve the path to the final target. Here’s a more detailed breakdown of what happens:

    1. **Starting at the Root**:
        - The kernel begins at the root directory (`/`). It checks if there is a dentry for the root directory in the dentry cache.
        - Since the root directory is accessed frequently, its dentry is almost always cached.

    2. **Moving to the Next Component (`home`)**:
        - The kernel next looks up the `home` directory under `/`. It checks the dentry cache for a dentry corresponding to `/home`.
        - If the dentry exists in the cache and is valid, the kernel uses it. If not, the kernel reads the inode for `home` from the disk, creates a new dentry for `home`, and caches it.

    3. **Continuing to `mralians`**:
        - The process repeats for the next component in the path. The kernel looks up the `mralians` directory under `/home`. Again, it checks if a dentry for `/home/mralians` exists in the cache.
        - If there is no cached dentry, the kernel reads the inode for `mralians` from the disk, creates a dentry, and caches it.

    4. **Accessing `dbus.sh`**:
        - Finally, the kernel looks up the file `dbus.sh` within `/home/mralians`. It checks if a dentry for `/home/mralians/dbus.sh` exists.
        - If the dentry is not in the cache, the kernel reads the inode for `dbus.sh` from disk, creates a dentry for it, and adds it to the cache.

    5. **File Operation**:
        - With the dentry for `dbus.sh` obtained, the kernel can perform the requested operation (like opening the file, reading data, etc.) based on the permissions and other inode information.

    ### Key Points:
    - **Dentry Caching**: This process leverages the dentry cache extensively to speed up file system navigation by avoiding redundant disk accesses for frequently accessed directories and files.
    - **Inode Lookups**: Whenever a dentry is not found in the cache, an inode lookup on the disk is necessary. This is more time-consuming than accessing cached data but is minimized by effective caching strategies.
    - **Path Resolution**: This is a fundamental part of the Linux filesystem architecture, allowing efficient file access even in deeply nested directory structures.

    This method efficiently handles file system access by balancing between memory usage (through caching) and the necessity to fetch fresh data from the disk when changes occur or the cache does not contain the needed entries.
11. let's clarify exactly how the Linux kernel uses both file names and inode numbers to locate files like `document.txt` in a directory like `/home/user`.

    Here’s a simplified step-by-step explanation focusing specifically on how file names are resolved to inode numbers, which is where the actual file data and metadata are stored:

    ### Directory Structure and File Lookup

    1. **Directory Representation**:
       - Every directory in Linux is represented as a special type of file that contains a list of directory entries.
       - Each directory has its own inode which, among other metadata, contains pointers to data blocks. These data blocks do not contain regular file data but directory data.

    2. **Contents of Directory Data Blocks**:
       - The data blocks of a directory inode contain entries; each entry associates a file name (like `document.txt`) with an inode number.
       - Essentially, these entries are pairs of `(file name, inode number)`.

    ### How the Kernel Finds `document.txt` in `/home/user`:

    1. **Starting at the Directory Inode**:
       - When a process requests a file located at `/home/user/document.txt`, the kernel starts by accessing the inode of the `/home/user` directory.
       
    2. **Reading Directory Contents**:
       - The kernel reads the data blocks pointed to by the inode of `/home/user`. These blocks contain the directory's entries.
       
    3. **Looking for `document.txt`**:
       - The kernel scans through these entries to find an entry where the file name is `document.txt`.
       - Each entry in this list looks something like this: `[file name] [inode number]`.
       
    4. **Accessing the Inode of `document.txt`**:
       - Once the entry for `document.txt` is found, the kernel takes the inode number from this entry.
       - Using this inode number, the kernel accesses the inode of `document.txt`. This inode contains all the metadata about the file, such as its size, permissions, timestamps, and pointers to the actual data blocks of the file.

    ### Summary

    - **File Name to Inode Resolution**:
       - The kernel uses the directory entries to resolve the file name (`document.txt`) to an inode number. This is how it knows which inode belongs to which file name within any given directory.
       
    - **Kernel's Use of Inodes**:
       - The inode is crucial because it contains the actual metadata and pointers to the data of the file. The file name itself is used primarily by users and user-space applications for convenience. The kernel uses the file name to look up the inode number in the directory’s contents, then it proceeds with inode-based operations.

    This mechanism ensures that the kernel can efficiently manage files and directories using inode numbers for all internal operations, while still providing a user-friendly way of accessing files via names. This separation of concerns (names for user interaction, inodes for system-level file management) allows for efficient and secure file system operations.
12. Yes, generally speaking, your explanation is correct, but it seems a bit mixed up in terms of terminology and the order of operations. Let me clarify and correct a few points:

    1. **Inode**: Each directory has an inode stored on disk. This inode contains metadata about the directory, such as permissions, timestamps, and pointers to data blocks.

    2. **Data Block**: The data block referenced by the directory's inode contains directory entries. These entries map filenames to the inodes of files or subdirectories within that directory.

    3. **Accessing a File**: When accessing a file like `/home/mralians/test.txt`, the kernel first checks the dentry cache (directory entry cache) to see if it has recently accessed the `/` directory (root directory). If not found in the cache, it will have to fetch the inode for the root directory from disk.

    4. **Traversing Directory Structure**: Once the kernel has the inode for `/`, it will then look into the data block associated with it to find the entry for `home`. It repeats this process of finding the inode and data block for each subsequent directory in the path (`mralians` and `test.txt`).

    5. **Dentry Cache**: As it traverses each directory, the kernel may populate the dentry cache with entries for directories and files it accesses frequently. This cache helps to speed up future lookups by avoiding repeated disk accesses.

    So, in summary, your explanation captures the essence of how filesystem traversal works, but it's a bit jumbled in terms of terminology and the sequence of actions.

## File System Types

### FFS

1. 
